# load packages
from operator import itemgetter

import loguru
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import (DirectoryLoader, PyPDFLoader,
                                                  UnstructuredMarkdownLoader)
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_core.output_parsers import StrOutputParser
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai.embeddings import OpenAIEmbeddings

# set logger
log = loguru.logger


class MarkdownVectorStore():
    def __init__(self, vector_store: DocArrayInMemorySearch, dir: str) -> None:
        """
        Initializes a new instance of the class.

        Args:
            vector_store (DocArrayInMemorySearch): The vector store to use.

        Returns:
            None
        """
        self.dir: str = dir
        self.loader: DirectoryLoader = DirectoryLoader(
            self.dir, 
            glob="**/*.md", 
            show_progress=True, 
            use_multithreading=True, 
            loader_cls=UnstructuredMarkdownLoader
        )
        self.embeddings = OllamaEmbeddings(model='qwen2')
        self.data = self.loader.load()
        self.vector_store: DocArrayInMemorySearch = vector_store(self.data, self.embeddings)
        self.retriever = self.vector_store.as_retriever()


class QAChatBot():
    def __init__(self, provider_key: str, model: str, vector_store: MarkdownVectorStore, prompt: PromptTemplate, parser=StrOutputParser) -> None:
        self.model: str = model
        self.llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(google_api_key=provider_key, model= model)
        self.prompt_template: PromptTemplate = prompt
        self.retriever: vector_store.retriever = vector_store.retriever
        self.parser: StrOutputParser = parser
        self.chain = (
            {
                "context": itemgetter("context") | self.retriever,
                "question": itemgetter("question")
            }
                | self.prompt_template
                | self.llm
                | self.parser
        )
        

    # @property
    # def set_model(self, model: str, provider_key: str) -> None:
    #     """
    #     Set the name of the model to use.

    #     Args:
    #         model (str): The name of the model to use.

    #     Returns:
    #         None
    #     """
    #     self.model = model
    #     self.llm = ChatGoogleGenerativeAI(google_api_key=provider_key, model= model)
    #     self.chain = self.llm
    #     self.add_prompts: dict = {}


    def invoke(self, prompt:str, parse: bool = True) -> str:
        """
        Invoke the ChatGoogleGenerativeAI to generate a response based on the provided prompt.
        
        Parameters:
            prompt (str): The input prompt for generating the response.
            """  """
        Returns:
            The response generated by the ChatGoogleGenerativeAI.
        """

        if parse:
            self.parser = StrOutputParser()
            self.chain = self.llm | self.parser 

        return self.chain.invoke(prompt)
    